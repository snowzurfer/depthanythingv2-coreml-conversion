{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from PIL import Image\n",
    "import coremltools as ct\n",
    "from huggingface_hub import hf_hub_download\n",
    "import tempfile\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch CoreMLTools\n",
    "\n",
    "We need to patch the lack of `upsample_bicubic2d`.\n",
    "Achieved by looking here:\n",
    "- https://github.com/pytorch/pytorch/blob/ac5f565fa7010bd77b9e779415e8709d347234b6/aten/src/ATen/native/UpSample.cpp#L10\n",
    "- https://github.com/pytorch/pytorch/blob/ac5f565fa7010bd77b9e779415e8709d347234b6/aten/src/ATen/native/UpSampleBicubic2d.cpp#L284\n",
    "- https://apple.github.io/coremltools/docs-guides/source/composite-operators.html#using-composite-ops-with-pytorch-conversion\n",
    "- https://github.com/huggingface/exporters/blob/7a545974275c7af167a2fa4e16c4574359f2acec/src/exporters/coreml/models.py#L530\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coremltools.converters.mil.frontend.torch.torch_op_registry import register_torch_op\n",
    "from coremltools.converters.mil.frontend.torch.ops import _get_inputs\n",
    "from coremltools.converters.mil import Builder as mb\n",
    "\n",
    "@register_torch_op\n",
    "def upsample_bicubic2d(context, node):\n",
    "    inputs = _get_inputs(context, node)\n",
    "\n",
    "    for input in inputs:\n",
    "        print(\"DBS upsample_bicubic2d, input: \", input)\n",
    "        if hasattr(input, 'name'):\n",
    "            print(\"DBS upsample_bicubic2d, input name: \", input.name)\n",
    "            if (input.name == '170' or input.name == '173') and hasattr(input, 'val'):\n",
    "                print(\"DBS upsample_bicubic2d, input value: \", input.val)\n",
    "        if hasattr(input, 'shape'):\n",
    "            print(\"DBS upsample_bicubic2d, input shape: \", input.shape)\n",
    "        if hasattr(input, 'dtype'):\n",
    "            print(\"DBS upsample_bicubic2d, input dtype: \", input.dtype)\n",
    "        if hasattr(input, 'type_str'):\n",
    "            print(\"DBS upsample_bicubic2d, input type_str: \", input.type_str)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    a = inputs[0]\n",
    "    b = inputs[3]\n",
    "    print(\"DBS upsample_bicubic2d, a: \", a)\n",
    "    print(\"DBS upsample_bicubic2d, b: \", b)\n",
    "    y = mb.resize_bilinear(\n",
    "        x=a, \n",
    "        target_size_height=int(b.val[0] * a.shape[2]), \n",
    "        target_size_width=int(b.val[1] * a.shape[3]), \n",
    "        name=node.name\n",
    "    )\n",
    "    context.add(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the DepthAnythingV2 model\n",
    "\n",
    "The `depth_anything_v2` folder I copied directly from their huggingface repo: https://huggingface.co/spaces/depth-anything/Depth-Anything-V2/tree/main/depth_anything_v2\n",
    "\n",
    "To make the code work, copy it / symlink it to the root of this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from depth_anything_v2.dpt import DepthAnythingV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"cat_dog.jpg\")\n",
    "image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This part I ported from the huggingface DepthAnythingV2 repo: https://huggingface.co/spaces/depth-anything/Depth-Anything-V2/blob/main/app.py\n",
    "\n",
    "Load the model and put it in `eval` mode.\n",
    "\n",
    "Change the `encoder` to the one of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "}\n",
    "encoder2name = {\n",
    "    'vits': 'Small',\n",
    "    'vitb': 'Base',\n",
    "    'vitl': 'Large',\n",
    "    'vitg': 'Giant', # UNAVAILABLE AS OF TODAY\n",
    "}\n",
    "encoder = 'vits'\n",
    "model_name = encoder2name[encoder]\n",
    "model = DepthAnythingV2(**model_configs[encoder])\n",
    "filepath = hf_hub_download(repo_id=f\"depth-anything/Depth-Anything-V2-{model_name}\", filename=f\"depth_anything_v2_{encoder}.pth\", repo_type=\"model\")\n",
    "state_dict = torch.load(filepath, map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util function to predict depth without grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_depth(image):\n",
    "    with torch.no_grad():\n",
    "        return model.infer_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that what we loaded works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.colormaps.get_cmap('Spectral_r')\n",
    "input_image = np.array(image)\n",
    "h, w = input_image.shape[:2]\n",
    "\n",
    "depth = predict_depth(input_image[:, :, ::-1])\n",
    "\n",
    "raw_depth = Image.fromarray(depth.astype('uint16'))\n",
    "tmp_raw_depth = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\n",
    "raw_depth.save(tmp_raw_depth.name)\n",
    "\n",
    "depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
    "depth = depth.astype(np.uint8)\n",
    "colored_depth = (cmap(depth)[:, :, :3] * 255).astype(np.uint8)\n",
    "colored_depth_image = Image.fromarray(colored_depth)\n",
    "\n",
    "gray_depth = Image.fromarray(depth)\n",
    "tmp_gray_depth = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\n",
    "gray_depth.save(tmp_gray_depth.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(gray_depth)\n",
    "display(colored_depth_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch the model output\n",
    "\n",
    "CoreMLTools wants tensors of rank 4 as output. However, DepthAnythingV2 returns a depth map of shap (1, H, W).\n",
    "Hence, we add a postprocessing step to fix this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(DepthModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output.unsqueeze(1)  # Add an extra dimension to make the output tensor of rank 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper = DepthModelWrapper(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally convert\n",
    "\n",
    "Notice the shape of `input_tensor`: its width is 518 because that's the default input size to DepthAnythingV2.\n",
    "The height can be any other value as long as it's a multiple of 14.\n",
    "For simplicity, we make the input have an aspect ratio of 1.\n",
    "Consequently, the output will have the same aspect ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(1, 3, 518, 518).to(device)\n",
    "traceable_model = torch.jit.trace(model_wrapper, input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image scale and bias for input image preprocessing\n",
    "scale = 1 / (0.226 * 255.0)\n",
    "bias = [-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225]\n",
    "\n",
    "# Define the input and output types for the CoreML model\n",
    "input_name = \"input\"\n",
    "output_name = \"output\"\n",
    "input_type = ct.ImageType(name=input_name, shape=input_tensor.shape, scale=scale, bias=bias,\n",
    "                          color_layout=ct.colorlayout.RGB)\n",
    "output_type = ct.ImageType(name=output_name, color_layout=ct.colorlayout.GRAYSCALE_FLOAT16)\n",
    "\n",
    "# Convert the PyTorch model to CoreML\n",
    "# If it gets stuck at this step: https://github.com/ggerganov/whisper.cpp/issues/773#issuecomment-1563324684\n",
    "mlmodel = ct.convert(\n",
    "    traceable_model,\n",
    "    inputs=[input_type],\n",
    "    outputs=[output_type],\n",
    "    minimum_deployment_target=ct.target.iOS16,\n",
    ")\n",
    "\n",
    "mlmodel.save(f\"DepthAnythingV2{model_name}.mlpackage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coreml-conversions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
